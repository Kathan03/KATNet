{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38f89c1-05ad-4e63-8acf-29c69d17eb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import htmd.ui as ht\n",
    "from moleculekit.molecule import Molecule\n",
    "import moleculekit.tools.voxeldescriptors as vd\n",
    "from moleculekit.tools.voxeldescriptors import getVoxelDescriptors, viewVoxelFeatures\n",
    "from moleculekit.tools.atomtyper import prepareProteinForAtomtyping\n",
    "from moleculekit.smallmol.smallmol import SmallMol\n",
    "from moleculekit.home import home\n",
    "import os\n",
    "\n",
    "import csv\n",
    "from tqdm import *\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as ps\n",
    "import multiprocessing as mp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from oddt import toolkit\n",
    "from oddt import datasets\n",
    "from oddt.datasets import pdbbind\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1c46b9-ca66-489f-90d1-099331784bd6",
   "metadata": {},
   "source": [
    "### SqueezeNet Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa799c01-3284-444e-8d05-e1c9092dc205",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "data_dir = \".\\DLSCORE-CNN-master\\DLSCORE-CNN-master\\dataset\"\n",
    "pdbbind_dir = os.path.join(data_dir, \"refined-set-2016\\\\\")\n",
    "pdbbind_dataset = pdbbind(home = pdbbind_dir, default_set='refined', version=2016)\n",
    "h5f = h5py.File(os.path.join(data_dir, \"data_1.h5\"), 'r')\n",
    "train_x_1, train_y = h5f['train_x_1'][:], h5f['train_y'][:]\n",
    "valid_x_1, valid_y = h5f['valid_x_1'][:], h5f['valid_y'][:]\n",
    "test_x_1, test_y = h5f['test_x_1'][:], h5f['test_y'][:]\n",
    "train_x_2, valid_x_2, test_x_2 = h5f['train_x_2'][:], h5f['valid_x_2'][:], h5f['test_x_2'][:]\n",
    "h5f.close()\n",
    "print(\"Data shapes: \", train_x_1.shape, valid_x_1.shape, test_x_1.shape, train_x_2.shape, valid_x_2.shape, test_x_2.shape)\n",
    "print(\"Y shape: \", train_y.shape, valid_y.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da906c71-d8d3-47ea-806a-0325abd5e46e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e388586d-5f1b-4b99-b8ce-00736755a401",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "\n",
    "class ExpandBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ExpandBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size=1, padding=0)\n",
    "        self.conv2 = nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        expand1 = F.relu(self.conv1(x))\n",
    "        expand2 = F.relu(self.conv2(x))\n",
    "        return torch.cat([expand1, expand2], dim=1)\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out_kq, d_out_v):\n",
    "        super().__init__()\n",
    "        self.d_out_kq=d_out_kq\n",
    "        self.W_query=nn.Parameter(torch.rand(d_in, d_out_kq))\n",
    "        self.W_key  = nn.Parameter(torch.rand(d_in, d_out_kq))\n",
    "        self.W_value=nn.Parameter(torch.rand(d_in, d_out_v))\n",
    "    \n",
    "    def forward(self, x_1, x_2):\n",
    "        queries_1=x_1.matmul(self.W_query)\n",
    "        keys_2=x_2.matmul(self.W_key)\n",
    "        values_2=x_2.matmul(self.W_value)\n",
    "        \n",
    "        attn_scores=queries_1.matmul(keys_2.T)\n",
    "        attn_weights=torch.softmax(\n",
    "            attn_scores/self.d_out_kq**0.5, dim=-1\n",
    "        )\n",
    "        \n",
    "        context_vec=attn_weights.matmul(values_2)\n",
    "        return context_vec\n",
    "\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "\n",
    "        self.conv0 = nn.Conv3d(16, 128,kernel_size = 3, stride=2, padding = 1)\n",
    "        \n",
    "        self.conv1 = nn.Conv3d(128, 256, kernel_size=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm3d(256)\n",
    "        self.pool1 = nn.MaxPool3d(kernel_size=3, stride=2)\n",
    "        self.expand_block1 = ExpandBlock(256, 64)\n",
    "        \n",
    "        self.conv2 = nn.Conv3d(128, 32, kernel_size=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm3d(32)\n",
    "        self.pool2 = nn.MaxPool3d(kernel_size=3, stride=2)\n",
    "        self.expand_block2 = ExpandBlock(32, 64)\n",
    "\n",
    "        self.conv3 = nn.Conv3d(128, 32, kernel_size=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm3d(32)\n",
    "        self.pool3 = nn.MaxPool3d(kernel_size=3, stride=2)\n",
    "        self.expand_block3 = ExpandBlock(32, 128)\n",
    "\n",
    "        #self.avg_pool_1 = nn.AvgPool3d(kernel_size=3, padding=1)\n",
    "\n",
    "\n",
    "        self.conv4 = nn.Conv3d(256, 32, kernel_size=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm3d(32)\n",
    "        self.pool4 = nn.MaxPool3d(kernel_size=2, stride=1, padding = 1)\n",
    "        self.expand_block4 = ExpandBlock(32, 128)\n",
    "\n",
    "        self.conv5 = nn.Conv3d(256, 128, kernel_size=1, padding = 1)\n",
    "        self.bn5 = nn.BatchNorm3d(128)\n",
    "        self.pool5 = nn.MaxPool3d(kernel_size=2, stride=1, padding = 1)\n",
    "        self.expand_block5 = ExpandBlock(128, 192)\n",
    "\n",
    "\n",
    "        self.conv6 = nn.Conv3d(384, 128, kernel_size=1, padding = 1)\n",
    "        self.bn6 = nn.BatchNorm3d(128)\n",
    "        self.pool6 = nn.MaxPool3d(kernel_size=2, stride=1, padding = 1)\n",
    "        self.expand_block6 = ExpandBlock(128, 32)\n",
    "\n",
    "        self.conv7 = nn.Conv3d(64, 16, kernel_size=1, padding = 1)\n",
    "        self.bn7 = nn.BatchNorm3d(16)\n",
    "        self.pool7 = nn.MaxPool3d(kernel_size=3, stride=1)\n",
    "        self.expand_block7 = ExpandBlock(16, 16)\n",
    "\n",
    "        \n",
    "        self.avg_pool_2 = nn.AvgPool3d(kernel_size=5)\n",
    "\n",
    "        self.crossattn = CrossAttention(256, 5, 256)\n",
    "\n",
    "\n",
    "        self.flatten = nn.Flatten(1) \n",
    "        \n",
    "        # self.fc_1 = nn.Linear(256, 512)\n",
    "        # self.fc_2 = nn.Linear(512, 720)\n",
    "        # self.fc_3 = nn.Linear(720, 1028)\n",
    "        # self.fc_4 = nn.Linear(1028, 512)\n",
    "        self.fc_5 = nn.Linear(256, 128)\n",
    "        self.fc_6 = nn.Linear(128, 32)\n",
    "        self.fc_7 = nn.Linear(32, 16)\n",
    "        self.fc_8 = nn.Linear(16, 1)\n",
    "                \n",
    "\n",
    "    def forward(self, x, x1):\n",
    "\n",
    "        x = F.relu(self.conv0(x))\n",
    "        \n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = self.expand_block1(x)\n",
    "        \n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = self.expand_block2(x)\n",
    "\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool3(x)\n",
    "        x = self.expand_block3(x)\n",
    "\n",
    "        #x = self.avg_pool_1(x)\n",
    "\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.pool4(x)\n",
    "        x = self.expand_block4(x)\n",
    "\n",
    "        x = F.relu(self.bn5(self.conv5(x)))\n",
    "        x = self.pool5(x)\n",
    "        x = self.expand_block5(x)\n",
    "\n",
    "        x = F.relu(self.bn6(self.conv6(x)))\n",
    "        x = self.pool6(x)\n",
    "        x = self.expand_block6(x)\n",
    "\n",
    "        x = F.relu(self.bn7(self.conv7(x)))\n",
    "        x = self.pool7(x)\n",
    "        x = self.expand_block7(x)\n",
    "\n",
    "        x = self.avg_pool_2(x)\n",
    "        \n",
    "        \n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = self.crossattn(x, x1)\n",
    "\n",
    "        #print(x.shape) #(batch_size x length) = (50, 512)\n",
    "        \n",
    "        # x = F.relu(self.fc_1(x))\n",
    "        # x = F.relu(self.fc_2(x))\n",
    "        # x = F.relu(self.fc_3(x))\n",
    "        # x = F.relu(self.fc_4(x))\n",
    "        x = F.relu(self.fc_5(x))\n",
    "        x = F.relu(self.fc_6(x))\n",
    "        x = F.relu(self.fc_7(x))\n",
    "        x = self.fc_8(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the model\n",
    "model = MyModel()\n",
    "summary(model, input_size=[(50, 16, 24, 24, 24), (50, 256,)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7d44c3-f384-471f-a0cb-1c80625dc3c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.handlers import ModelCheckpoint, EarlyStopping\n",
    "from ignite.metrics import Loss\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "X1_train_tensor = torch.tensor(train_x_1, dtype=torch.float32)\n",
    "X2_train_tensor = torch.tensor(train_x_2, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(train_y, dtype=torch.float32)\n",
    "\n",
    "X1_valid_tensor = torch.tensor(valid_x_1, dtype=torch.float32)\n",
    "X2_valid_tensor = torch.tensor(valid_x_2, dtype=torch.float32)\n",
    "y_valid_tensor = torch.tensor(valid_y, dtype=torch.float32)\n",
    "\n",
    "\n",
    "X1_test_tensor = torch.tensor(test_x_1, dtype=torch.float32)\n",
    "X2_test_tensor = torch.tensor(test_x_2, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(test_y, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = TensorDataset(X1_train_tensor, X2_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=50, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(X1_test_tensor, X2_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=50, shuffle=False)\n",
    "\n",
    "valid_dataset = TensorDataset(X1_valid_tensor, X2_valid_tensor, y_valid_tensor)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=50, shuffle=False)\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, amsgrad=True)\n",
    "\n",
    "\n",
    "best_loss = np.inf\n",
    "patience = 10\n",
    "counter = 0\n",
    "\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "# Lists to store training and validation losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "\n",
    "ep = []\n",
    "l = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    for input1, input2, labels in train_loader:\n",
    "        optimizer.zero_grad()  # Zero the parameter gradients\n",
    "        #print(input1.shape)\n",
    "        #print(input1.shape)\n",
    "        outputs = model(input1, input2)  # Forward pass\n",
    "        loss = criterion(outputs, labels)  # Calculate the loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Optimize\n",
    "        running_loss += loss.item() * input1.size(0)\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    l.append(epoch_loss)\n",
    "    ep.append(epoch+1)\n",
    "    train_losses.append(epoch_loss)\n",
    "    #print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "    \n",
    "\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for input1, input2, labels in valid_loader:\n",
    "            outputs = model(input1, input2)\n",
    "            val_loss = criterion(outputs, labels)\n",
    "            running_val_loss += val_loss.item() * input1.size(0)\n",
    "        epoch_val_loss = running_val_loss / len(valid_loader.dataset)\n",
    "        val_losses.append(epoch_val_loss)\n",
    "    \n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, Val Loss: {epoch_val_loss:.4f}\")\n",
    "\n",
    "    # Check for early stopping\n",
    "    if epoch_val_loss < best_loss:\n",
    "        best_loss = epoch_val_loss\n",
    "        counter = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pth')  # Save the best model\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75deccc1-dec0-469b-b493-a542c110bcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(ep, l, color='red', label='Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.title(f'Training Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1dae63-495d-41cd-8aa8-f53e3a7a0cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "model.eval()  # Set model to evaluation mode\n",
    "test_loss = 0.0\n",
    "yhat = []\n",
    "y = []\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "with torch.no_grad():\n",
    "    for input1, input2, label in test_loader:\n",
    "        output = model(input1, input2)\n",
    "        yhat.append(output.numpy().reshape(-1))\n",
    "        y.append(label.numpy().reshape(-1))\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item() * input1.size(0)\n",
    "    avg_test_loss = test_loss / len(test_dataset)\n",
    "    print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
    "\n",
    "print(len(yhat))\n",
    "print(len(y))\n",
    "\n",
    "y = np.concatenate(y).reshape(-1)\n",
    "yhat = np.concatenate(yhat).reshape(-1)\n",
    "\n",
    "y = np.flip(y)\n",
    "\n",
    "evaluations ={'RMSE': sklearn.metrics.root_mean_squared_error(y, yhat),\n",
    "        'MAE': sklearn.metrics.mean_absolute_error(y, yhat),\n",
    "        'Max Error': sklearn.metrics.max_error(y, yhat),\n",
    "        'CORR': np.corrcoef(y, yhat),\n",
    "    }\n",
    "print(evaluations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae8f451-c51a-4311-9f05-2d42f7208481",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7cbf3c-ffc9-4632-917b-683b1d672e1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e437b3a6-b7e7-494b-abef-a71949af83af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr, linregress\n",
    "\n",
    "# Calculate Pearson correlation coefficient\n",
    "corr_coefficient, _ = pearsonr(yhat, y)\n",
    "RMSE = sklearn.metrics.root_mean_squared_error(y, yhat)\n",
    "\n",
    "# Calculate line of best fit\n",
    "slope, intercept, _, _, _ = linregress(y, yhat)\n",
    "line = slope * y + intercept\n",
    "\n",
    "# Plotting\n",
    "plt.scatter(y, yhat)\n",
    "plt.plot(y, line, color='red', label='Correlation')\n",
    "plt.xlabel('Experimental pK')\n",
    "plt.ylabel('Predicted pK')\n",
    "plt.title(f'Pearson Correlation Coefficient: {corr_coefficient:.4f}, RMSE: {RMSE}')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2828f1-470b-4e74-930f-f2eb6a1e9b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation losses per epoch\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss per Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7e8442-4c71-4db9-84c9-51c334574650",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
