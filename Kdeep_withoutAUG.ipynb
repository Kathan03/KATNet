{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4e28ca4-e2a8-461e-8931-799b6f003457",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\katha\\anaconda3\\envs\\my_gpu\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.17.0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490fe4d9-6855-4fb0-8f2d-a9eaed5a06ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import htmd.ui as ht\n",
    "from moleculekit.molecule import Molecule\n",
    "import moleculekit.tools.voxeldescriptors as vd\n",
    "from moleculekit.tools.voxeldescriptors import getVoxelDescriptors, viewVoxelFeatures\n",
    "from moleculekit.tools.atomtyper import prepareProteinForAtomtyping\n",
    "from moleculekit.smallmol.smallmol import SmallMol\n",
    "from moleculekit.home import home\n",
    "import os\n",
    "\n",
    "import csv\n",
    "from tqdm import *\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as ps\n",
    "import multiprocessing as mp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from oddt import toolkit\n",
    "from oddt import datasets\n",
    "from oddt.datasets import pdbbind\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3189c783-cd84-491f-a9fd-3c39baf7dc82",
   "metadata": {},
   "source": [
    "### Training and Initializing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fba669e-3c7a-4107-9a64-1cc3cd4b3821",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "data_dir = \".\\DLSCORE-CNN-master\\DLSCORE-CNN-master\\dataset\"\n",
    "pdbbind_dir = os.path.join(data_dir, \"refined-set-2016\\\\\")\n",
    "pdbbind_dataset = pdbbind(home = pdbbind_dir, default_set='refined', version=2016)\n",
    "h5f = h5py.File(os.path.join(data_dir, \"data_1.h5\"), 'r')\n",
    "train_x_1, train_y = h5f['train_x_1'][:], h5f['train_y'][:]\n",
    "valid_x_1, valid_y = h5f['valid_x_1'][:], h5f['valid_y'][:]\n",
    "test_x_1, test_y = h5f['test_x_1'][:], h5f['test_y'][:]\n",
    "train_x_2, valid_x_2, test_x_2 = h5f['train_x_2'][:], h5f['valid_x_2'][:], h5f['test_x_2'][:]\n",
    "h5f.close()\n",
    "print(\"Data shapes: \", train_x_1.shape, valid_x_1.shape, test_x_1.shape, train_x_2.shape, valid_x_2.shape, test_x_2.shape)\n",
    "print(\"Y shape: \", train_y.shape, valid_y.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a981fc5-8d49-4151-a727-58396d0bb3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming train_y is your array\n",
    "# Example data, replace with your actual data\n",
    "\n",
    "# Define the number of buckets (bins)\n",
    "num_bins = 10  # You can adjust the number of bins as needed\n",
    "\n",
    "# Create the histogram data\n",
    "counts, bin_edges = np.histogram(train_y, bins=num_bins, range=(2, 12))\n",
    "\n",
    "# Compute the bin centers\n",
    "bin_centers = 0.5 * (bin_edges[:-1] + bin_edges[1:])\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.bar(bin_centers, counts, width=(bin_edges[1] - bin_edges[0]), edgecolor='black')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of train_y')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071cd07d-8efd-4258-8f4c-665c5a12ef7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "\n",
    "# from keras.models import Sequential,Model\n",
    "# from keras.layers import Dropout, Flatten, Dense, Input, Add, merge, concatenate\n",
    "# from keras.layers.convolutional import Conv3D\n",
    "# from keras.layers.pooling import MaxPooling3D, GlobalAveragePooling3D, AveragePooling3D\n",
    "# from keras.initializers import he_uniform\n",
    "# from keras.initializers import glorot_uniform\n",
    "\n",
    "\n",
    "# from keras.models import Sequential,Model\n",
    "# from keras.layers import Dropout, Flatten, Dense, Input, Add, merge, concatenate\n",
    "# from keras.layers.convolutional import Conv3D\n",
    "# from keras.layers.pooling import MaxPooling3D, GlobalAveragePooling3D, AveragePooling3D\n",
    "# from keras import metrics\n",
    "# from keras import optimizers\n",
    "# from keras.utils import plot_model\n",
    "# from keras import backend as K\n",
    "# from keras.utils.training_utils import multi_gpu_model\n",
    "# from keras.utils.data_utils import Sequence\n",
    "# from keras.callbacks import ModelCheckpoint\n",
    "# from keras.initializers import he_uniform\n",
    "# from keras.initializers import glorot_uniform\n",
    "\n",
    "class ExpandBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ExpandBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size=1, padding=0)\n",
    "        self.conv2 = nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        expand1 = F.relu(self.conv1(x))\n",
    "        expand2 = F.relu(self.conv2(x))\n",
    "        return torch.cat([expand1, expand2], dim=1)\n",
    "\n",
    "\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "\n",
    "        self.conv0 = nn.Conv3d(16, 128,kernel_size = 3, stride=2, padding = 1)\n",
    "        \n",
    "        self.conv1 = nn.Conv3d(128, 256, kernel_size=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm3d(256)\n",
    "        self.pool1 = nn.MaxPool3d(kernel_size=3, stride=2)\n",
    "        self.expand_block1 = ExpandBlock(256, 64)\n",
    "        \n",
    "        self.conv2 = nn.Conv3d(128, 32, kernel_size=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm3d(32)\n",
    "        self.pool2 = nn.MaxPool3d(kernel_size=3, stride=2)\n",
    "        self.expand_block2 = ExpandBlock(32, 64)\n",
    "\n",
    "        self.conv3 = nn.Conv3d(128, 32, kernel_size=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm3d(32)\n",
    "        self.pool3 = nn.MaxPool3d(kernel_size=3, stride=2)\n",
    "        self.expand_block3 = ExpandBlock(32, 128)\n",
    "\n",
    "        #self.avg_pool_1 = nn.AvgPool3d(kernel_size=3, padding=1)\n",
    "\n",
    "\n",
    "        self.conv4 = nn.Conv3d(256, 32, kernel_size=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm3d(32)\n",
    "        self.pool4 = nn.MaxPool3d(kernel_size=2, stride=1, padding = 1)\n",
    "        self.expand_block4 = ExpandBlock(32, 128)\n",
    "\n",
    "        self.conv5 = nn.Conv3d(256, 128, kernel_size=1, padding = 1)\n",
    "        self.bn5 = nn.BatchNorm3d(128)\n",
    "        self.pool5 = nn.MaxPool3d(kernel_size=2, stride=1, padding = 1)\n",
    "        self.expand_block5 = ExpandBlock(128, 192)\n",
    "\n",
    "\n",
    "        self.conv6 = nn.Conv3d(384, 128, kernel_size=1, padding = 1)\n",
    "        self.bn6 = nn.BatchNorm3d(128)\n",
    "        self.pool6 = nn.MaxPool3d(kernel_size=2, stride=1, padding = 1)\n",
    "        self.expand_block6 = ExpandBlock(128, 32)\n",
    "\n",
    "        self.conv7 = nn.Conv3d(64, 16, kernel_size=1, padding = 1)\n",
    "        self.bn7 = nn.BatchNorm3d(16)\n",
    "        self.pool7 = nn.MaxPool3d(kernel_size=3, stride=1)\n",
    "        self.expand_block7 = ExpandBlock(16, 16)\n",
    "\n",
    "        \n",
    "        self.avg_pool_2 = nn.AvgPool3d(kernel_size=5)\n",
    "\n",
    "        #self.crossattn = CrossAttention(256, 5, 256)\n",
    "\n",
    "\n",
    "        self.flatten = nn.Flatten(1) \n",
    "        \n",
    "        # self.fc_1 = nn.Linear(256, 512)\n",
    "        # self.fc_2 = nn.Linear(512, 720)\n",
    "        # self.fc_3 = nn.Linear(720, 1028)\n",
    "        # self.fc_4 = nn.Linear(1028, 512)\n",
    "        self.fc_5 = nn.Linear(256, 128)\n",
    "        self.fc_6 = nn.Linear(128, 32)\n",
    "        self.fc_7 = nn.Linear(32, 16)\n",
    "        self.fc_8 = nn.Linear(16, 1)\n",
    "                \n",
    "\n",
    "    def forward(self, x, x1):\n",
    "\n",
    "        x = F.relu(self.conv0(x))\n",
    "        \n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = self.expand_block1(x)\n",
    "        \n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = self.expand_block2(x)\n",
    "\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool3(x)\n",
    "        x = self.expand_block3(x)\n",
    "\n",
    "        #x = self.avg_pool_1(x)\n",
    "\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.pool4(x)\n",
    "        x = self.expand_block4(x)\n",
    "\n",
    "        x = F.relu(self.bn5(self.conv5(x)))\n",
    "        x = self.pool5(x)\n",
    "        x = self.expand_block5(x)\n",
    "\n",
    "        x = F.relu(self.bn6(self.conv6(x)))\n",
    "        x = self.pool6(x)\n",
    "        x = self.expand_block6(x)\n",
    "\n",
    "        x = F.relu(self.bn7(self.conv7(x)))\n",
    "        x = self.pool7(x)\n",
    "        x = self.expand_block7(x)\n",
    "\n",
    "        x = self.avg_pool_2(x)\n",
    "        \n",
    "        \n",
    "        x = self.flatten(x)\n",
    "\n",
    "        #x = self.crossattn(x, x1)\n",
    "\n",
    "        #print(x.shape) #(batch_size x length) = (50, 512)\n",
    "        \n",
    "        # x = F.relu(self.fc_1(x))\n",
    "        # x = F.relu(self.fc_2(x))\n",
    "        # x = F.relu(self.fc_3(x))\n",
    "        # x = F.relu(self.fc_4(x))\n",
    "        x = F.relu(self.fc_5(x))\n",
    "        x = F.relu(self.fc_6(x))\n",
    "        x = F.relu(self.fc_7(x))\n",
    "        x = self.fc_8(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the model\n",
    "model = MyModel()\n",
    "summary(model, input_size=[(50, 16, 24, 24, 24), (50, 256,)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115c9b70-ae89-4dea-89a6-a73ea9a967a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.handlers import ModelCheckpoint, EarlyStopping\n",
    "from ignite.metrics import Loss\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "X1_train_tensor = torch.tensor(train_x_1, dtype=torch.float32)\n",
    "X2_train_tensor = torch.tensor(train_x_2, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(train_y, dtype=torch.float32)\n",
    "\n",
    "X1_valid_tensor = torch.tensor(valid_x_1, dtype=torch.float32)\n",
    "X2_valid_tensor = torch.tensor(valid_x_2, dtype=torch.float32)\n",
    "y_valid_tensor = torch.tensor(valid_y, dtype=torch.float32)\n",
    "\n",
    "\n",
    "X1_test_tensor = torch.tensor(test_x_1, dtype=torch.float32)\n",
    "X2_test_tensor = torch.tensor(test_x_2, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(test_y, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = TensorDataset(X1_train_tensor, X2_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=50, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(X1_test_tensor, X2_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=50, shuffle=False)\n",
    "\n",
    "valid_dataset = TensorDataset(X1_valid_tensor, X2_valid_tensor, y_valid_tensor)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=50, shuffle=False)\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, amsgrad=True)\n",
    "\n",
    "\n",
    "best_loss = np.inf\n",
    "patience = 10\n",
    "counter = 0\n",
    "\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "# Lists to store training and validation losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "\n",
    "ep = []\n",
    "l = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    for input1, input2, labels in train_loader:\n",
    "        optimizer.zero_grad()  # Zero the parameter gradients\n",
    "        #print(input1.shape)\n",
    "        #print(input1.shape)\n",
    "        outputs = model(input1, input2)  # Forward pass\n",
    "        loss = criterion(outputs, labels)  # Calculate the loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Optimize\n",
    "        running_loss += loss.item() * input1.size(0)\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    l.append(epoch_loss)\n",
    "    ep.append(epoch+1)\n",
    "    train_losses.append(epoch_loss)\n",
    "    #print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "    \n",
    "\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for input1, input2, labels in valid_loader:\n",
    "            outputs = model(input1, input2)\n",
    "            val_loss = criterion(outputs, labels)\n",
    "            running_val_loss += val_loss.item() * input1.size(0)\n",
    "        epoch_val_loss = running_val_loss / len(valid_loader.dataset)\n",
    "        val_losses.append(epoch_val_loss)\n",
    "    \n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, Val Loss: {epoch_val_loss:.4f}\")\n",
    "\n",
    "    # Check for early stopping\n",
    "    if epoch_val_loss < best_loss:\n",
    "        best_loss = epoch_val_loss\n",
    "        counter = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pth')  # Save the best model\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4062c602-a76e-4ad5-8b6e-68d478f1d572",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(ep, l, color='red', label='Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.title(f'Training Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52918c0f-ff12-4387-8333-619284a1b136",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "model.eval()  # Set model to evaluation mode\n",
    "test_loss = 0.0\n",
    "yhat = []\n",
    "y = []\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "with torch.no_grad():\n",
    "    for input1, input2, label in test_loader:\n",
    "        output = model(input1, input2)\n",
    "        yhat.append(output.numpy().reshape(-1))\n",
    "        y.append(label.numpy().reshape(-1))\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item() * input1.size(0)\n",
    "    avg_test_loss = test_loss / len(test_dataset)\n",
    "    print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
    "\n",
    "print(len(yhat))\n",
    "print(len(y))\n",
    "\n",
    "y = np.concatenate(y).reshape(-1)\n",
    "yhat = np.concatenate(yhat).reshape(-1)\n",
    "\n",
    "y = np.flip(y)\n",
    "\n",
    "evaluations ={'RMSE': sklearn.metrics.root_mean_squared_error(y, yhat),\n",
    "        'MAE': sklearn.metrics.mean_absolute_error(y, yhat),\n",
    "        'Max Error': sklearn.metrics.max_error(y, yhat),\n",
    "        'CORR': np.corrcoef(y, yhat),\n",
    "    }\n",
    "print(evaluations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a077c1-ab9d-4b43-9027-6d93ebd4dafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr, linregress\n",
    "\n",
    "# Calculate Pearson correlation coefficient\n",
    "corr_coefficient, _ = pearsonr(yhat, y)\n",
    "RMSE = sklearn.metrics.root_mean_squared_error(y, yhat)\n",
    "\n",
    "# Calculate line of best fit\n",
    "slope, intercept, _, _, _ = linregress(y, yhat)\n",
    "line = slope * y + intercept\n",
    "\n",
    "# Plotting\n",
    "plt.scatter(y, yhat)\n",
    "plt.plot(y, line, color='red', label='Correlation')\n",
    "plt.xlabel('Experimental pK')\n",
    "plt.ylabel('Predicted pK')\n",
    "plt.title(f'Pearson Correlation Coefficient: {corr_coefficient:.4f}, RMSE: {RMSE}')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2343d0d3-5dc9-4a9f-840a-e7b042dce214",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
